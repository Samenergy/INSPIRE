# I.N.S.P.I.R.E. ğŸš€

### Intelligent Network System for Partnerships, Insights, Research & Expansion  

**AI-powered B2B Intelligence Platform for MSMEs in Rwanda**

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-18.2-blue.svg)](https://reactjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-blue.svg)](https://www.typescriptlang.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Project Overview](#-project-overview)
- [Key Features](#-key-features)
- [System Architecture](#-system-architecture)
- [Technology Stack](#-technology-stack)
- [Installation & Setup](#-installation--setup)
- [Configuration](#-configuration)
- [Usage Guide](#-usage-guide)
- [API Documentation](#-api-documentation)
- [Project Structure](#-project-structure)
- [Development](#-development)
- [Deployment](#-deployment)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ Project Overview

**I.N.S.P.I.R.E.** is an enterprise-grade AI-powered B2B intelligence platform designed to help Micro, Small & Medium Enterprises (MSMEs) in Rwanda discover strategic partnership opportunities, analyze market trends, and make data-driven business decisions.

The platform integrates multiple data sources, advanced AI/ML models, and an intuitive web-based dashboard to deliver actionable intelligence tailored for the Rwandan MSME ecosystem.

### What I.N.S.P.I.R.E. Does

1. **Scrapes** company data from multiple sources (Google/SerpAPI)
2. **Classifies** articles based on SME objectives (95.2% accuracy)
3. **Analyzes** companies using RAG (Retrieval-Augmented Generation)
4. **Generates** personalized outreach campaigns
5. **Visualizes** insights through comprehensive dashboards

---

## âœ¨ Key Features

### ğŸ¤– AI/ML Capabilities

- **Article Classification** (95.2% accuracy)
  - ML-based classification using SentenceTransformer
  - Categorizes articles as: Directly Relevant, Indirectly Useful, Not Relevant
  - Personalized based on SME objectives

- **RAG Analysis** (10 Intelligence Categories)
  - Latest Updates (product launches, financial results, partnerships)
  - Challenges (competitive pressures, operational difficulties)
  - Decision Makers (executives, leaders with roles)
  - Market Position (competitors, market share, advantages)
  - Future Plans (expansion, investments, strategic initiatives)
  - Action Plan (3 specific steps for SME engagement)
  - Solution (3 relevant SME solutions for company needs)
  - Company Info (5-sentence company description)
  - Strengths (key competitive advantages)
  - Opportunities (potential growth areas)

### ğŸ“Š Data Management

- **Comprehensive Company Profiles**
  - Multi-source data aggregation
  - RAG-extracted intelligence fields
  - Industry classification and location tracking

- **Article Management**
  - Automated scraping from Google/SerpAPI
  - Content classification and relevance scoring
  - Sentiment analysis

- **Analysis Storage**
  - Structured JSON storage for RAG results
  - Historical analysis tracking
  - Confidence scores per category

### ğŸ“§ Outreach Campaigns

- **Automated Campaign Generation**
  - Email, Call, and Meeting templates
  - Personalized based on company analysis
  - SME objective-driven content

- **Campaign Management**
  - Draft, Scheduled, Sent, Completed statuses
  - Campaign history and tracking
  - Company-specific campaign organization

### ğŸ“ˆ Dashboard & Analytics

- **Real-time Statistics**
  - Total companies, articles, analyses, campaigns
  - Companies by status (completed, loading, pending, failed)
  - Articles by classification
  - Campaigns by type and status
  - Industry distribution

- **Data Visualization**
  - Interactive charts (Bar, Doughnut, Line)
  - Recent activity feed
  - Performance metrics

### ğŸ” Security & Authentication

- **JWT-based Authentication**
  - Secure signup and login
  - Token-based API access
  - Protected routes

- **SME Management**
  - Profile management
  - Sector and objective tracking
  - Multi-tenant data isolation

---

## ğŸ—ï¸ System Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React Frontendâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  FastAPI Backendâ”‚
â”‚   (Port 3000)   â”‚  HTTP   â”‚   (Port 8000)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                â”‚                â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
            â”‚    MySQL     â”‚  â”‚    Redis    â”‚  â”‚   Celery   â”‚
            â”‚  (Port 3306) â”‚  â”‚ (Port 6379) â”‚  â”‚   Workers  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚    Milvus    â”‚  â”‚    Ollama    â”‚
                    â”‚ (Port 19530) â”‚  â”‚ (Port 11434) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **User Action** â†’ Frontend sends request to FastAPI
2. **API Processing** â†’ FastAPI validates and routes request
3. **Background Task** â†’ Celery worker processes long-running tasks
4. **Data Storage** â†’ MySQL stores structured data
5. **Vector Search** â†’ Milvus stores embeddings for RAG
6. **LLM Processing** â†’ Ollama generates AI responses
7. **Progress Tracking** â†’ Redis stores task progress
8. **Response** â†’ Results returned to frontend

---

## ğŸ› ï¸ Technology Stack

### Backend

| Technology | Version | Purpose |
|------------|---------|---------|
| **Python** | 3.11 | Core language |
| **FastAPI** | 0.104.1 | Web framework |
| **Celery** | 5.3.4 | Background task processing |
| **SQLAlchemy** | 2.0.23 | ORM |
| **MySQL** | 8.0 | Primary database |
| **Redis** | 7 | Caching & task queue |
| **Milvus** | Latest | Vector database |
| **Ollama** | Latest | LLM inference (Llama 3.1) |
| **SentenceTransformer** | 2.2.2 | Embeddings |
| **scikit-learn** | 1.3.2 | ML models |
| **Playwright** | 1.40.0 | Web scraping |
| **SerpAPI** | - | Google search API |

### Frontend

| Technology | Version | Purpose |
|------------|---------|---------|
| **React** | 18.2 | UI framework |
| **TypeScript** | 5.3.2 | Type safety |
| **Vite** | 7.2.2 | Build tool |
| **Material-UI** | 5.14.18 | Component library |
| **Chart.js** | 4.4.0 | Data visualization |
| **React Router** | 6.20.0 | Navigation |
| **Tailwind CSS** | 3.3.5 | Styling |
| **Framer Motion** | 12.6.3 | Animations |

---

## ğŸš€ Installation & Setup

### Prerequisites

- **Node.js** >= 18.x
- **Python** >= 3.10
- **MySQL** >= 8.0
- **Redis** >= 7.0
- **Docker** & **Docker Compose** (optional, for containerized deployment)
- **Ollama** (for LLM inference)

### Quick Start with Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/yourusername/INSPIRE.git
cd INSPIRE

# Start all services
cd Backend
docker-compose up -d

# Wait for services to initialize (30-60 seconds)
# Backend will be available at http://localhost:8000
# Frontend will be available at http://localhost:80 (via Nginx)
```

### Manual Setup

#### Backend Setup

```bash
# Navigate to backend directory
cd Backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium

# Create .env file (see Configuration section)
cp .env.example .env
# Edit .env with your configuration

# Initialize database
python -m app.database_init

# Start FastAPI server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### Frontend Setup

```bash
# Navigate to frontend directory
cd Frontend

# Install dependencies
npm install

# Start development server
npm run dev
# Frontend will be available at http://localhost:3000
```

#### Celery Worker Setup

```bash
# In Backend directory with virtual environment activated
celery -A app.celery_app worker --loglevel=info --concurrency=1
```

#### Ollama Setup

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama service
ollama serve

# Pull Llama 3.1 model (in another terminal)
ollama pull llama3.1:8b-instruct-q4_K_M
```

#### Milvus Setup (Optional)

```bash
# Using Docker
docker run -d --name milvus-standalone \
  -p 19530:19530 \
  -p 9091:9091 \
  milvusdb/milvus:latest

# Or use docker-compose (included in Backend/docker-compose.yml)
```

---

## âš™ï¸ Configuration

### Backend Environment Variables

Create a `.env` file in the `Backend/` directory:

```env
# Application
APP_NAME=Inspire
APP_VERSION=1.0.0
DEBUG=True
LOG_LEVEL=INFO

# Database
DB_NAME=inspire
DB_USER=root
DB_PASSWORD=your_password
DB_HOST=localhost
DB_PORT=3306
MYSQL_URL=mysql+pymysql://root:your_password@localhost:3306/inspire

# Redis
REDIS_URL=redis://localhost:6379/0

# API Keys
SERPAPI_API_KEY=your_serpapi_key
APIFY_API_KEY=your_apify_key  # Optional
APIFY_API_TOKEN=your_apify_token  # Optional
LINKEDIN_COOKIE=your_linkedin_cookie  # Optional

# Milvus (Vector Database)
MILVUS_HOST=localhost
MILVUS_PORT=19530

# Ollama (LLM)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b-instruct-q4_K_M

# RAG Hyperparameters
RAG_TEMPERATURE=0.3
RAG_TOP_K=5
RAG_CHUNK_SIZE=500
RAG_CHUNK_OVERLAP=100

# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=3600
MAX_CONCURRENT_SCRAPES=5
```

### Frontend Configuration

The frontend API base URL is configured in:
- `Frontend/src/services/authService.ts` (default: `http://127.0.0.1:8000`)

For local development, update to:
```typescript
const API_BASE_URL = 'http://localhost:8000/api';
```

---

## ğŸ“– Usage Guide

### 1. User Registration & Login

1. Navigate to the signup page
2. Enter your SME details (name, email, password)
3. Complete your profile (sector, objectives)
4. You'll be automatically logged in

### 2. Adding a Company

1. Go to **Companies** page
2. Click **Add Company**
3. Enter company name and location
4. Click **Analyze** to start the unified analysis pipeline

### 3. Running Unified Analysis

The unified analysis pipeline:
1. **Scrapes** company data from Google (SerpAPI)
2. **Classifies** articles based on your SME objectives
3. **Runs RAG analysis** to extract 10 intelligence categories
4. **Stores** everything in the database

**Progress Tracking:**
- Analysis runs in the background (Celery task)
- Track progress via the progress endpoint
- Results appear automatically when complete

### 4. Viewing Company Intelligence

1. Navigate to **Companies** page
2. Click on a company card
3. View:
   - **Articles** (classified by relevance)
   - **Analysis** (RAG-extracted intelligence)
   - **Company Info** (5-sentence description)
   - **Strengths** (competitive advantages)
   - **Opportunities** (growth areas)

### 5. Generating Outreach Campaigns

1. Open a company profile
2. Click **Generate Campaign**
3. Select outreach type (Email, Call, Meeting)
4. Review and edit generated content
5. Save as draft or mark as sent

### 6. Dashboard Analytics

The dashboard shows:
- Total companies, articles, analyses, campaigns
- Companies by status
- Articles by classification
- Campaigns by type and status
- Recent activity feed

---

## ğŸ“š API Documentation

### Base URL

- **Production**: `http://127.0.0.1:8000`
- **Local**: `http://localhost:8000`

### Interactive API Docs

- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

### Key Endpoints

#### Authentication

```http
POST /api/auth/signup
POST /api/auth/login
GET  /api/auth/me
POST /api/auth/verify-token
PUT  /api/auth/profile
```

#### Companies

```http
GET    /api/inspire/companies
POST   /api/inspire/companies
GET    /api/inspire/companies/{company_id}
PUT    /api/inspire/companies/{company_id}
DELETE /api/inspire/companies/{company_id}
GET    /api/inspire/companies/{company_id}/analysis
GET    /api/inspire/companies/{company_id}/articles
GET    /api/inspire/companies/{company_id}/intelligence
```

#### Unified Analysis

```http
POST /api/v1/unified/unified-analysis
GET  /api/v1/unified/unified-analysis/progress/{job_id}
GET  /api/v1/unified/unified-analysis/result/{job_id}
```

#### Outreach Campaigns

```http
POST   /api/outreach/generate
GET    /api/outreach/campaigns
GET    /api/outreach/campaigns/{campaign_id}
PUT    /api/outreach/campaigns/{campaign_id}
PUT    /api/outreach/campaigns/{campaign_id}/status
DELETE /api/outreach/campaigns/{campaign_id}
```

#### Dashboard

```http
GET /api/inspire/dashboard/stats
GET /api/inspire/dashboard/activity
```

### Authentication

All protected endpoints require a JWT token in the Authorization header:

```http
Authorization: Bearer <your_jwt_token>
```

---

## ğŸ“ Project Structure

```
Cappp/
â”‚
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI application entry point
â”‚   â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”‚   â”œâ”€â”€ models.py               # Pydantic models
â”‚   â”‚   â”œâ”€â”€ middleware.py          # Request middleware
â”‚   â”‚   â”œâ”€â”€ logging_config.py      # Logging setup
â”‚   â”‚   â”œâ”€â”€ celery_app.py          # Celery configuration
â”‚   â”‚   â”œâ”€â”€ database_init.py       # Database initialization
â”‚   â”‚   â”œâ”€â”€ database_mysql_inspire.py  # MySQL connection & operations
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ routers/               # API route handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py            # Authentication endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ comprehensive.py   # Scraping endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ unified_analysis.py # Unified analysis pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_analysis.py    # RAG analysis endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ inspire_database.py # Database CRUD endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ outreach.py        # Campaign endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ advanced_classification.py # Article classification
â”‚   â”‚   â”‚   â””â”€â”€ summarization.py   # Text summarization
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ services/              # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_analysis_service.py      # RAG implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ advanced_model_service.py    # Classification models
â”‚   â”‚   â”‚   â”œâ”€â”€ comprehensive_scrape_service.py # Scraping orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ outreach_service.py          # Campaign generation
â”‚   â”‚   â”‚   â”œâ”€â”€ auth_service.py               # Authentication logic
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ scrapers/              # Web scraping modules
â”‚   â”‚   â”‚   â”œâ”€â”€ serpapi_scraper.py # SerpAPI integration
â”‚   â”‚   â”‚   â”œâ”€â”€ apify_scraper.py   # Apify integration
â”‚   â”‚   â”‚   â””â”€â”€ base.py            # Base scraper class
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ tasks/                  # Celery background tasks
â”‚   â”‚   â”‚   â””â”€â”€ unified_analysis_task.py # Unified analysis pipeline
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ utils/                  # Utility functions
â”‚   â”‚
â”‚   â”œâ”€â”€ ml_models/                  # Trained ML models
â”‚   â”‚   â”œâ”€â”€ classification/        # Classification models
â”‚   â”‚   â””â”€â”€ summarization/          # Summarization models
â”‚   â”‚
â”‚   â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”‚   â”‚   â”œâ”€â”€ ML_Model_Notebook.ipynb
â”‚   â”‚   â””â”€â”€ RAG_10Categories_Complete.ipynb
â”‚   â”‚
â”‚   â”œâ”€â”€ exports/                    # CSV exports
â”‚   â”œâ”€â”€ logs/                       # Application logs
â”‚   â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile                   # Docker image definition
â”‚   â”œâ”€â”€ docker-compose.yml          # Docker Compose configuration
â”‚   â””â”€â”€ nginx.conf                  # Nginx configuration
â”‚
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx                 # Main application component
â”‚   â”‚   â”œâ”€â”€ main.tsx                # React entry point
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ components/             # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ auth/               # Authentication components
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard/          # Dashboard components
â”‚   â”‚   â”‚   â”œâ”€â”€ accounts/           # Company management
â”‚   â”‚   â”‚   â”œâ”€â”€ campaigns/          # Campaign management
â”‚   â”‚   â”‚   â”œâ”€â”€ layout/             # Layout components
â”‚   â”‚   â”‚   â”œâ”€â”€ notifications/      # Notification system
â”‚   â”‚   â”‚   â””â”€â”€ ui/                 # Reusable UI components
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ context/                 # React Context providers
â”‚   â”‚   â”‚   â”œâ”€â”€ AuthContext.tsx     # Authentication state
â”‚   â”‚   â”‚   â””â”€â”€ ThemeContext.tsx    # Theme management
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ services/               # API service layer
â”‚   â”‚   â”‚   â”œâ”€â”€ authService.ts      # Authentication API
â”‚   â”‚   â”‚   â”œâ”€â”€ companyService.ts   # Company API
â”‚   â”‚   â”‚   â””â”€â”€ AnalyticsDataService.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ utils.ts                # Utility functions
â”‚   â”‚
â”‚   â”œâ”€â”€ public/                     # Static assets
â”‚   â”œâ”€â”€ package.json                # Node.js dependencies
â”‚   â”œâ”€â”€ vite.config.ts              # Vite configuration
â”‚   â”œâ”€â”€ tsconfig.json               # TypeScript configuration
â”‚   â””â”€â”€ tailwind.config.js          # Tailwind CSS configuration
â”‚
â”œâ”€â”€ ERD.png                         # Entity Relationship Diagram
â”œâ”€â”€ System Architecture.png         # System architecture diagram
â”œâ”€â”€ Sequence Diagram.png             # Sequence diagram
â”œâ”€â”€ Usecase.png                     # Use case diagram
â””â”€â”€ README.md                        # This file
```

---

## ğŸ”§ Development

### Running in Development Mode

#### Backend

```bash
cd Backend
source venv/bin/activate
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### Frontend

```bash
cd Frontend
npm run dev
```

#### Celery Worker

```bash
cd Backend
source venv/bin/activate
celery -A app.celery_app worker --loglevel=info --concurrency=1
```

### Code Quality

#### Backend

```bash
# Format code
black app/

# Sort imports
isort app/

# Lint
flake8 app/

# Type checking
mypy app/
```

#### Frontend

```bash
# Lint
npm run lint

# Type check
npm run build:check
```

### Database Migrations

The database schema is managed through `database_init.py`. To update the schema:

1. Modify the table creation logic in `Backend/app/database_init.py`
2. Run the initialization script:
   ```bash
   python -m app.database_init
   ```

### Testing

```bash
# Backend tests
cd Backend
pytest

# Frontend tests
cd Frontend
npm test
```

---

## ğŸš¢ Deployment

### Docker Deployment

The project includes a complete Docker Compose setup:

```bash
cd Backend
docker-compose up -d
```

This starts:
- FastAPI backend
- Celery worker
- MySQL database
- Redis cache
- Milvus vector database
- Ollama LLM service
- Nginx reverse proxy

### Production Considerations

1. **Environment Variables**: Use secure environment variable management
2. **Database**: Use managed MySQL/PostgreSQL service
3. **Redis**: Use managed Redis service
4. **SSL/TLS**: Configure SSL certificates for HTTPS
5. **Monitoring**: Set up logging and monitoring (e.g., Sentry, DataDog)
6. **Backups**: Regular database backups
7. **Scaling**: Use multiple Celery workers for high load

### Environment-Specific Configuration

- **Development**: `DEBUG=True`, local services
- **Staging**: `DEBUG=False`, staging database
- **Production**: `DEBUG=False`, production database, SSL enabled

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Style

- Follow PEP 8 for Python code
- Use TypeScript for frontend code
- Write meaningful commit messages
- Add docstrings to functions and classes
- Include type hints in Python code

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **SentenceTransformer** for embeddings
- **Ollama** for LLM inference
- **FastAPI** for the excellent web framework
- **Material-UI** for the component library
- **Chart.js** for data visualization

---

## ğŸ“ Support & Contact

- **API Documentation**: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)
- **Issues**: [GitHub Issues](https://github.com/yourusername/INSPIRE/issues)
- **Email**: support@inspire.software

---

## ğŸ¯ Roadmap

- [ ] Enhanced LinkedIn scraping integration
- [ ] Real-time notifications via WebSockets
- [ ] Advanced analytics and reporting
- [ ] Mobile app (React Native)
- [ ] Multi-language support
- [ ] Integration with CRM systems
- [ ] Advanced ML model fine-tuning
- [ ] Automated email sending
- [ ] Calendar integration for meetings

---

**Built with â¤ï¸ for MSMEs in Rwanda**
